# Blackjack Reinforcement Learning

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

SystÃ¨me complet d'apprentissage par renforcement pour comparer des stratÃ©gies naÃ¯ves et avec comptage de cartes au Blackjack.

## ğŸ“Š RÃ©sultats ClÃ©s

- **12 agents RL implÃ©mentÃ©s** : Q-Learning, SARSA, Monte Carlo, DQN
- **Meilleur agent** : Monte Carlo Count (46.5% de taux de victoire)
- **Meilleur optimisÃ©** : SARSA (43.5%, proche de la stratÃ©gie de base optimale Ã  42.68%)
- **234 configurations testÃ©es** via recherche de grille hyperparamÃ¨tres
- **Interface Streamlit interactive** pour jouer contre les agents

## ğŸ“š Documentation

- **[Rapport dÃ©taillÃ©](rapport.pdf)** : Analyse complÃ¨te du projet (~40 pages)
- **[PrÃ©sentation](presentation.pdf)** : Slides de prÃ©sentation (~25 slides)
- **[Documentation ReadTheDocs](docs/)** : Guide utilisateur et documentation technique
- **[Guide LaTeX](LATEX_README.md)** : Instructions de compilation des documents

## ğŸ¯ Objectif

Comparer deux approches de jeu au Blackjack :
1. **StratÃ©gie NaÃ¯ve** : Agents apprennent sans information sur le comptage des cartes
2. **StratÃ©gie avec Comptage** : Agents utilisent le systÃ¨me Hi-Lo pour optimiser leurs dÃ©cisions

## ğŸš€ Quick Start

### Installation

```bash
# Cloner le projet
git clone https://github.com/votre-username/P3_Blackjack_RL.git
cd P3_Blackjack_RL

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# Installer le package en mode dÃ©veloppement
pip install -e .
```

### Lancer l'Interface Streamlit

```bash
streamlit run streamlit_app/main.py
```

L'application offre 3 pages :
- **Vue d'ensemble** : PrÃ©sentation du projet et rÃ©sultats
- **Comparaison** : Comparaison dÃ©taillÃ©e des agents
- **Jouer** : Jouer au Blackjack avec recommandations des agents

### EntraÃ®nement d'un Agent

```bash
# EntraÃ®ner un agent Q-Learning naÃ¯f
python scripts/train_naive.py --config config/agents_naive/qlearning.yaml

# EntraÃ®ner un agent SARSA naÃ¯f
python scripts/train_naive.py --config config/agents_naive/sarsa.yaml

# EntraÃ®ner un agent Monte Carlo naÃ¯f
python scripts/train_naive.py --config config/agents_naive/mc.yaml
```

### GÃ©nÃ©rer les Graphiques

```bash
python scripts/generate_results_plots.py
```

### Compiler la Documentation LaTeX

```bash
# Windows PowerShell
.\compile_latex.ps1

# Ou manuellement
pdflatex rapport.tex
pdflatex presentation.tex
```

## ğŸ“Š RÃ©sultats Attendus

| Agent | Type | Win Rate (NaÃ¯f) | Win Rate (Comptage) | AmÃ©lioration |
|-------|------|-----------------|---------------------|--------------|
| **Monte Carlo** | Tabular | â‰¥42% | â‰¥44% | +2-4% |
| **Q-Learning** | Tabular | â‰¥42% | â‰¥45% | +3-5% |
| **SARSA** | Tabular | â‰¥42% | â‰¥45% | +3-5% |
| **DQN** | Deep RL | â‰¥38% | â‰¥42% | +4-6% |

## ğŸ—ï¸ Architecture

```
P3_Blackjack_RL/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/       # Environnement Blackjack + comptage cartes
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ naive/        # Agents sans comptage (MC, Q-Learning, SARSA, DQN)
â”‚   â”‚   â””â”€â”€ counting/     # Agents avec comptage
â”‚   â”œâ”€â”€ training/         # Pipeline d'entraÃ®nement
â”‚   â”œâ”€â”€ evaluation/       # Ã‰valuation et comparaison
â”‚   â””â”€â”€ utils/            # Utilitaires (logging, config)
â”œâ”€â”€ streamlit_app/        # Interface web Streamlit
â”œâ”€â”€ config/               # Configurations YAML
â”œâ”€â”€ data/                 # ModÃ¨les entraÃ®nÃ©s et rÃ©sultats
â”œâ”€â”€ tests/                # Tests unitaires
â””â”€â”€ scripts/              # Scripts d'entraÃ®nement CLI
```

## ğŸ“ Algorithmes ImplÃ©mentÃ©s

### Agents NaÃ¯fs

1. **Monte Carlo** : Apprentissage par Ã©pisodes complets
   - First-visit MC avec moyennage des returns
   - Ã‰tat : (player_sum, dealer_card, usable_ace)
   
2. **Q-Learning** : TD control off-policy
   - Update : Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
   - Exploration : Îµ-greedy avec decay
   
3. **SARSA** : TD control on-policy
   - Update : Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
   - Plus conservateur que Q-Learning
   
4. **DQN** : Deep Q-Network avec replay buffer
   - Neural network pour approximation Q-values
   - Target network pour stabilitÃ©

### SystÃ¨me de Comptage Hi-Lo

```python
# Valeurs des cartes
Low cards (2-6):  +1
Neutral (7-9):     0
High cards (10-A): -1

# True Count = Running Count / Decks Remaining
```

## ğŸ“ˆ Interface Streamlit (En dÃ©veloppement)

```bash
# Lancer l'interface
streamlit run streamlit_app/app.py
```

**Pages disponibles** :
1. ğŸ  **Accueil** : Documentation et guide
2. ğŸ“ **Training** : EntraÃ®nement interactif
3. ğŸ“Š **Comparison** : Comparaison agents
4. ğŸ® **Simulation** : Jouer contre l'agent
5. ğŸƒ **Card Counting** : Analyse du comptage
6. ğŸ“ˆ **Dashboard** : Vue d'ensemble

## ğŸ§ª Tests

Le projet maintient une couverture de tests â‰¥80% :

```bash
# Tests rapides
pytest tests/ -v --tb=short

# Tests avec rapport dÃ©taillÃ©
pytest tests/ -v --cov=src --cov-report=term-missing

# Tests d'un module spÃ©cifique
pytest tests/test_card_counting.py -v -s
```

## ğŸ“š Configuration

Les agents sont configurÃ©s via fichiers YAML :

```yaml
# config/agents_naive/qlearning.yaml
agent:
  type: qlearning
  state_dim: 3

hyperparameters:
  alpha: 0.01
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_decay: 0.9995

training:
  episodes: 250000
  eval_frequency: 5000
```

## ğŸ”¬ DÃ©veloppement

### Standards de Code

- **Formatage** : Black (line length 100)
- **Type hints** : mypy --strict
- **Docstrings** : Google format
- **Tests** : pytest avec â‰¥80% coverage

### PrÃ©-commit

```bash
# Formatter le code
black src/ streamlit_app/ tests/
isort src/ streamlit_app/ tests/

# Linting
flake8 src/ streamlit_app/ tests/ --max-line-length 100

# Type checking
mypy src/ --strict

# Tests
pytest tests/ --cov=src
```

## ğŸ“Š RÃ©sultats Scientifiques

Le comptage de cartes Hi-Lo amÃ©liore significativement la performance :
- **AmÃ©lioration moyenne** : +3-5% win rate
- **P-value** : < 0.05 (diffÃ©rence significative)
- **Cohen's d** : > 0.5 (effet modÃ©rÃ©)

## ğŸ“„ License

MIT License - Voir [LICENSE](LICENSE)

## ğŸ¤ Contributions

Ce projet suit strictement le [PROJECT_GUIDE.md](PROJECT_GUIDE.md) pour toutes les implÃ©mentations.

## ğŸ“ Contact

Pour questions ou suggestions, crÃ©er une issue sur le repository.

---

**Status** : âœ… Phase 1-2 complÃ¨tes (Environnement + Agents naÃ¯fs fonctionnels)  
**Prochaines Ã©tapes** : Agents avec comptage, Interface Streamlit, Ã‰valuation complÃ¨te
