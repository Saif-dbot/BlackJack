# Q-Learning with Card Counting configuration
agent:
  type: qlearning_count
  state_dim: 4  # (player_sum, dealer_card, usable_ace, true_count_bin)
  action_dim: 2  # STAND=0, HIT=1

hyperparameters:
  alpha: 0.1              # Learning rate
  gamma: 1.0              # No discounting for episodic tasks
  epsilon_start: 1.0      # Initial exploration rate
  epsilon_min: 0.05       # Minimum exploration rate
  epsilon_decay: 0.99995  # Slower decay for better exploration

training:
  episodes: 300000        # More episodes for card counting (larger state space)
  eval_frequency: 5000    # Evaluate every N episodes
  eval_episodes: 1000     # Number of evaluation episodes
  save_frequency: 10000   # Save checkpoint every N episodes
  seed: 42                # Random seed

environment:
  deck_type: finite       # 'finite' or 'infinite' (counting only works with finite)
  num_decks: 6            # Number of decks
  natural: true           # Natural blackjack pays 1.5x
  sab: true               # Stand after bust
  enable_counting: true   # Enable card counting
